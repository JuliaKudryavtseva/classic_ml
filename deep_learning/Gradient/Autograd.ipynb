{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "SEED=42\n",
    "def seed_everything(SEED=42):\n",
    "    random.seed(SEED)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmarks = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[0.8823],\n",
      "        [0.9150],\n",
      "        [0.3829],\n",
      "        [0.9593],\n",
      "        [0.3904]], requires_grad=True)\n",
      "w:\n",
      " tensor([[0.6009],\n",
      "        [0.2566],\n",
      "        [0.7936],\n",
      "        [0.9408],\n",
      "        [0.1332]], requires_grad=True)\n",
      "\n",
      "f = wTx:  2.0232625007629395\n",
      "df/dw = [df/dw1, df/dw2, ... , df/dwn]^T = x with shape torch.Size([5, 1]): True\n",
      "df/dx = [df/dx1, df/dx2, ... , df/dxn]^T = w with shape torch.Size([5, 1]): True\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "VECTOR_LENGTH = (5, 1)\n",
    "\n",
    "x = torch.rand(VECTOR_LENGTH, requires_grad=True)\n",
    "w = torch.rand(VECTOR_LENGTH, requires_grad=True)\n",
    "f = torch.matmul(w.T, x)\n",
    "\n",
    "f.backward()\n",
    "w.grad\n",
    "\n",
    "print('x:\\n', x)\n",
    "print('w:\\n', w)\n",
    "print()\n",
    "print('f = wTx: ', f.item())\n",
    "print('df/dw = [df/dw1, df/dw2, ... , df/dwn]^T = x with shape {dim:}: {prof:}'.format(dim=x.shape, prof=all(x == w.grad)))\n",
    "print('df/dx = [df/dx1, df/dx2, ... , df/dxn]^T = w with shape {dim:}: {prof:}'.format(dim=w.shape, prof=all(w == x.grad)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "predicitons = Wx: R^5 -> R^20\n",
      "\n",
      "dWx / dx = \n",
      "[[dw_1Tx / dw_1, dw_1Tx / dw_2, ... , dw_1Tx / dx_n]\n",
      " [dw_2Tx / dw_1, dw_2Tx / dw_2, ... , dw_2Tx / dw_n]\n",
      "... \n",
      " [dw_mTx / dw_1, dw_mTx / dw_2, ... , dw_mTx / dw_n]]\n",
      "\n",
      "Dimentions: ones: (20, 1), x: (5, 1), dWx / dx: torch.Size([20, 5])\n",
      "dWx / dx = ones.T xT :  True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd.functional import jacobian\n",
    "seed_everything()\n",
    "\n",
    "ROWS, COLUMNS = (20, 5)\n",
    "WEIGHTS_DIM = (ROWS, COLUMNS)\n",
    "VECTOR_LENGTH = (COLUMNS, 1)\n",
    "\n",
    "W = torch.rand(WEIGHTS_DIM, requires_grad=True)\n",
    "x = torch.rand(VECTOR_LENGTH, requires_grad=False)\n",
    "\n",
    "def predict(W, x):\n",
    "    return torch.matmul(W, x)\n",
    "\n",
    "pred = predict(W, x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_jac(weights, x):\n",
    "    unit_vectors = torch.eye(ROWS)\n",
    "    jacobian_rows = [torch.autograd.grad(predict(weights, x.squeeze(1)), weights, vec)[0]\n",
    "                     for vec in unit_vectors]\n",
    "    return torch.stack(jacobian_rows).sum(0)\n",
    "\n",
    "print('forward\\npredicitons = Wx: R^{dim1} -> R^{dim2}\\n'.format(dim1=x.size(0), dim2=pred.size(0)))\n",
    "\n",
    "\n",
    "# backward\n",
    "\n",
    "jac = torch.matmul(torch.ones((ROWS, 1)), x.T)\n",
    "print('dWx / dx = \\n[[dw_1Tx / dw_1, dw_1Tx / dw_2, ... , dw_1Tx / dx_n]\\n [dw_2Tx / dw_1, dw_2Tx / dw_2, ... , dw_2Tx / dw_n]\\n... \\n [dw_mTx / dw_1, dw_mTx / dw_2, ... , dw_mTx / dw_n]]\\n')\n",
    "print('Dimentions: ones: {dim_1}, x: {dim_x}, dWx / dx: {dim3}'.format(dim_1=(ROWS, 1), dim_x=VECTOR_LENGTH, dim3=jac.shape))\n",
    "print('dWx / dx = ones.T xT : ', all((jac == compute_jac(W, x)).flatten()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient for Loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d MSE / d pred_i = - 1/n * 2 (y_i - pred_i)\n",
      "d MSE / d pred = [d MSE / d pred_1, d MSE / d pred_2, ... , d MSE / d pred_n]^T:  True\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "\n",
    "VECTOR_LENGTH = (5, 1)\n",
    "\n",
    "pred = torch.rand(VECTOR_LENGTH, requires_grad=True)\n",
    "y = torch.rand(VECTOR_LENGTH)\n",
    "\n",
    "mse = torch.mean((y - pred) ** 2)\n",
    "mse.backward()\n",
    "\n",
    "\n",
    "n = VECTOR_LENGTH[0]\n",
    "grad_mse = - 2/n * (y - pred)\n",
    "\n",
    "print('d MSE / d pred_i = - 1/n * 2 (y_i - pred_i)')\n",
    "print('d MSE / d pred = [d MSE / d pred_1, d MSE / d pred_2, ... , d MSE / d pred_n]^T: ',  all(grad_mse == pred.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple backpropagation\n",
    "```\n",
    "forward:\n",
    "pred = Wx\n",
    "MSE(y, pred)\n",
    "\n",
    "backward:\n",
    "dMSE/dW = 1/M {Sum_i dERROR_i/dW for i in range(m)}\n",
    "dERROR_i/dW = (d ERROR_i / d pred_i)*(d pred_i / dW) \n",
    "[m * n] = [1 * 1] * [m * n]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "predicitons = Wx: R^5 -> R^20\n",
      "Expected dimention:  (20, 5)\n",
      "\n",
      "dMSE / dW:  torch.Size([20, 5])\n",
      "dMSE / dW = SUM_i ( dMSE / dpred_i * dpred_i / dW ) for i in range(ROWS):  True\n",
      "Dimentions:  torch.Size([1]) torch.Size([20, 20, 5])\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "ROWS, COLUMNS = (20, 5)\n",
    "VECTOR_LENGTH = (COLUMNS, 1)\n",
    "\n",
    "x = torch.rand(VECTOR_LENGTH)\n",
    "W = torch.rand((ROWS, COLUMNS), requires_grad=True)\n",
    "n = ROWS\n",
    "y = torch.rand((ROWS, 1))\n",
    "\n",
    "print('forward\\npredicitons = Wx: R^{dim1} -> R^{dim2}'.format(dim1=x.size(0), dim2=y.size(0)))\n",
    "print('Expected dimention: ', (ROWS, COLUMNS))\n",
    "print()\n",
    "\n",
    "# forward\n",
    "pred = torch.matmul(W, x)\n",
    "mse = torch.mean((y - pred) ** 2)\n",
    "\n",
    "W.zero_\n",
    "mse.backward()\n",
    "print('dMSE / dW: ', W.grad.shape)\n",
    "\n",
    "# backward\n",
    "dMSE_dpred = -2/n * (y - pred)\n",
    "\n",
    "jacobian_predictions = torch.zeros((ROWS, ROWS, COLUMNS))\n",
    "for i in range(ROWS):\n",
    "    jacobian_predictions[i][i] = x.squeeze(1)\n",
    "\n",
    "    \n",
    "jac = torch.zeros((ROWS, COLUMNS))\n",
    "for i in range(ROWS):\n",
    "    jac += dMSE_dpred[i] * jacobian_predictions[i]\n",
    "   \n",
    "print('dMSE / dW = SUM_i ( dMSE / dpred_i * dpred_i / dW ) for i in range(ROWS): ', all((jac == W.grad).flatten()))\n",
    "print('Dimentions: ', dMSE_dpred[0].shape, jacobian_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduct backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dMSE_dpred = -2/n * (y - pred)\n",
    "jac2 = torch.matmul(dMSE_dpred, x.T)\n",
    "all((jac2 == W.grad).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network\n",
    "\n",
    "```\n",
    "forward:\n",
    "x_1 = W_1 x_0\n",
    "[m, 1] = [n * m] [n * 1]\n",
    "\n",
    "pred = W_2 x_1\n",
    "[n, 1] = [n * m] [m * 1]\n",
    "\n",
    "MSE(y, pred)\n",
    "[1] = [1, n][n, 1]\n",
    "\n",
    "backward:\n",
    "dMSE/dW1 = [[dMSE/dpred]T [dpred/dx_1]]T x_0.T \n",
    "[m * n] = [1 * 1] * [m * n]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected dimention:  (9, 5)\n",
      "\n",
      "torch.Size([1, 7]) -> torch.Size([1, 9]) -> torch.Size([5, 9])\n",
      "to gradient:  torch.Size([9, 5])\n",
      "torch.Size([9, 5]) \n",
      " True\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "ROWS1, COLUMNS1 = (9, 5)\n",
    "ROWS2, COLUMNS2 = (7, 9)\n",
    "\n",
    "VECTOR_LENGTH = (COLUMNS, 1)\n",
    "\n",
    "x = torch.rand(VECTOR_LENGTH)\n",
    "W1 = torch.rand((ROWS1, COLUMNS1), requires_grad=True)\n",
    "W2 = torch.rand((ROWS2, COLUMNS2), requires_grad=True)\n",
    "\n",
    "n = ROWS2\n",
    "# print('forward\\npredicitons = Wx: R^{dim1} -> R^{dim2} -> R^{dim3}'.format(dim1=x.size(0), dim2=, dim3=y.size(0)))\n",
    "print('Expected dimention: ', (ROWS1, COLUMNS1))\n",
    "print()\n",
    "y = torch.rand((ROWS2, 1))\n",
    "\n",
    "# forward\n",
    "pred = torch.matmul(W2, torch.matmul(W1,x))\n",
    "mse = torch.mean((y - pred) ** 2)\n",
    "\n",
    "W.zero_\n",
    "mse.backward()\n",
    "\n",
    "# Dervatives = Gradient.T\n",
    "dMSE_dpred = -2/ROWS2 * (y - pred).T\n",
    "dMSE_dx1 = torch.matmul(dMSE_dpred, W2)\n",
    "dMSE_dW1 = torch.matmul(x, dMSE_dx1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(dMSE_dpred.shape, '->', dMSE_dx1.shape, '->', dMSE_dW1.shape)\n",
    "print('to gradient: ', dMSE_dW1.T.shape)\n",
    "print(W1.grad.shape, '\\n', all((dMSE_dW1.T == W1.grad).flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
